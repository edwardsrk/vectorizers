{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "#This implementation produces a sparse representation of the counts using\n",
    "#scipy.sparse.csr_matrix.\n",
    "#If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "#that does some kind of feature selection then the number of features will\n",
    "#be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "hashing = HashingVectorizer()\n",
    "\"\"\"Convert a collection of text documents to a matrix of token occurrences\n",
    "\n",
    "It turns a collection of text documents into a scipy.sparse matrix holding\n",
    "token occurrence counts (or binary occurrence information), possibly\n",
    "normalized as token frequencies if norm='l1' or projected on the euclidean\n",
    "unit sphere if norm='l2'.\n",
    "\n",
    "This text vectorizer implementation uses the hashing trick to find the\n",
    "token string name to feature integer index mapping.\n",
    "\n",
    "This strategy has several advantages:\n",
    "\n",
    "- it is very low memory scalable to large datasets as there is no need to\n",
    "  store a vocabulary dictionary in memory\n",
    "\n",
    "- it is fast to pickle and un-pickle as it holds no state besides the\n",
    "  constructor parameters\n",
    "\n",
    "- it can be used in a streaming (partial fit) or parallel pipeline as there\n",
    "  is no state computed during fit.\n",
    "\n",
    "There are also a couple of cons (vs using a CountVectorizer with an\n",
    "in-memory vocabulary):\n",
    "\n",
    "- there is no way to compute the inverse transform (from feature indices to\n",
    "  string feature names) which can be a problem when trying to introspect\n",
    "  which features are most important to a model.\n",
    "\n",
    "- there can be collisions: distinct tokens can be mapped to the same\n",
    "  feature index. However in practice this is rarely an issue if n_features\n",
    "  is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "\n",
    "- no IDF weighting as this would render the transformer stateful.\n",
    "\n",
    "The hash function employed is the signed 32-bit version of Murmurhash3.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
